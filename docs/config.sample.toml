# Sample of a configuration. Copy this file and edit it.
#
# For `ampd solo`: place this file at `.amp/config.toml` for auto-discovery,
# or pass `--config <path>` / set the `AMP_CONFIG` env var explicitly.
# For other commands (`server`, `worker`, `controller`): `--config` or `AMP_CONFIG` is required.
#
# See config.md for more context.
#
# Note that using a config file is not mandatory. You can alternatively provide some or all values
# through env vars. See the 'Using env vars' section of config.md for how to do this.
#
# When using filesystem paths, any relative paths will be resolved from the directory of this file.

# ================================
# Top-level configuration fields
# ================================

# Storage paths
# Where the extracted datasets are stored.
data_dir = "data"

# Path to a providers directory. Each provider is configured as a separate toml file in this
# directory. Dataset definitions will reference providers by a relative path to this directory.
providers_dir = "providers"

# Path to a directory containing dataset manifest files.
manifests_dir = "manifests"

# Memory and performance
# max_mem_mb = 0            # Global memory limit for all queries in MB. 0 means unlimited (default: 0)
# query_max_mem_mb = 0      # Per-query memory limit in MB. 0 means unlimited per-query (default: 0)
# spill_location = []       # Paths for DataFusion temporary files for spill-to-disk (default: [])

# Operational timing
# poll_interval_secs = 1.0                 # Polling interval for new blocks during dump in seconds (default: 1.0)
# microbatch_max_interval = 100000         # Max interval for derived dataset dump microbatches in blocks (default: 100000)
# server_microbatch_max_interval = 1000    # Max interval for streaming server microbatches in blocks (default: 1000)
# keep_alive_interval = 30                 # Keep-alive interval for streaming server in seconds (default: 30, minimum: 30)

# Service addresses
# flight_addr = "0.0.0.0:1602"      # Arrow Flight RPC server address (default: "0.0.0.0:1602")
# jsonl_addr = "0.0.0.0:1603"       # JSON Lines server address (default: "0.0.0.0:1603")
# admin_api_addr = "0.0.0.0:1610"   # Admin API server address (default: "0.0.0.0:1610")

# ================================
# Table sections
# ================================

# Database configuration
# The `url` field is REQUIRED for all commands (server, worker, controller, migrate).
# Solo mode automatically provides a managed PostgreSQL URL by default. You can
# optionally provide your own database URL (via this config file or the
# AMP_CONFIG_METADATA_DB__URL environment variable) to use an external database
# instead, which skips the managed PostgreSQL startup. The managed instance is
# initialized and persisted in `.amp/metadb/`, enabling zero-config solo mode operation.
[metadata_db]
url = "postgres://<pg_url>"
# pool_size = 10       # Size of the connection pool (default: 10)
# auto_migrate = true  # Automatically run database migrations on startup (default: true)

# Observability
[opentelemetry]
# metrics_url = "http://localhost:4318/v1/metrics"     # Remote OpenTelemetry metrics collector endpoint (binary HTTP)
# metrics_export_interval_secs = 60.0                  # Interval in seconds to export metrics
# trace_url = "http://localhost:4318/v1/traces"        # Remote OpenTelemetry traces collector endpoint (HTTP)
# trace_ratio = 1.0                                    # Ratio of traces to sample (default: 1.0)

# Writer/Parquet configuration
[writer]
# compression = "zstd(1)"    # Compression algorithm: zstd, lz4, gzip, brotli, snappy, uncompressed (default: zstd(1))
# bloom_filters = false      # Enable bloom filters (default: false)
# cache_size_mb = 1024       # Parquet metadata cache size in MB (default: 1024)
# max_row_group_mb = 512     # Max row group size in MB (default: 512)

# Target partition size configuration
# Overflow is a tuning parameter that provides some flexibility for some cases,
# e.g. if two files that are both slightly larger than 50% of the size limit won't
# compact because their total exceeds the limit. Overflow provides some flexibility
# in this case. Additionally, the segment size algorithm is based on an approximation
# of the future compacted segment size which might end up being smaller or larger
# depending on factors like table width, value cardinality, compression ratios, etc.
# So adjusting Overflow provides a mechanism to reflect the realities of the data
# at rest while configuring the compaction algorithm.
# overflow = "1"         # Overflow multiplier: 1x target size (default: "1"), can use "1.5" for 1.5x, etc.
# bytes = 2147483648     # Target bytes per file - 2GB (default: 2147483648)
# rows = 0               # Target rows per file, 0 means no limit (default: 0)
# File compactor
[writer.compactor]
# active = false                  # Enable or disable the compactor (default: false)
# metadata_concurrency = 2        # Max concurrent metadata operations (default: 2)
# write_concurrency = 2           # Max concurrent compaction write operations (default: 2)
# min_interval = 1.0              # Interval in seconds to run the compactor (default: 1.0)
# cooldown_duration = 1024.0      # Base cooldown duration in seconds (default: 1024.0)
# overflow = "1"                  # Eager compaction overflow (default: "1")
# bytes = 0                       # Eager compaction byte threshold (default: 0)
# rows = 0                        # Eager compaction row threshold (default: 0)

# Garbage collector
[writer.collector]
# active = false                   # Enable or disable the collector (default: false)
# min_interval = 30.0              # Interval in seconds to run the garbage collector (default: 30.0)
# deletion_lock_duration = 1800.0  # Duration in seconds to hold deletion lock on compacted files (default: 1800.0 = 30 minutes)
