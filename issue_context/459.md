# Move input_batch_size_blocks into StreamingQuery

Microbatch size control can be a StreamingQuery responsibility. Steps:

1. Add a test for input_batch_size_blocks
2. Refactor it into StreamingQuery
3. Rename to microbatch_max_interval or something

# 1. Add a test for input_batch_size_blocks
## Objective
Test that the `input_batch_size_blocks` parameter correctly controls the number of Parquet files created during SQL dataset dumps. Specifically, verify that setting `input_batch_size_blocks=1` when dumping 4 blocks results in 4 separate files.

## Test Overview

### Test Name
`sql_dataset_input_batch_size`

### Location
`/tests/src/tests.rs` (implemented)

### Dataset to Test
- **Dataset**: `sql_over_eth_firehose`
- **Table**: `even_blocks`
- **Source**: Filters even blocks from `eth_firehose`

### Test Parameters
- **input_batch_size_blocks**: 1
- **Block range**: 4 blocks (15_000_000 to 15_000_003)
- **Expected result**: 4 separate Parquet files

## Implementation Status

### ‚úÖ Completed
- Added `dump_dataset_with_batch_size` helper function to `test_support.rs`
- Made `catalog_for_dataset` function public 
- Made `TestEnv.dataset_store` field public
- Implemented the test function in `tests.rs`
- Uses `sql_over_eth_firehose` dataset instead of `sql_stream_ds` to avoid dependency issues

### ‚ùå Current Issue
The test is implemented but failing due to blessed data limitations:
- Blessed data only contains block 15000000
- Test tries to process blocks 15000000-15000003
- Error: "failed to resolve block metadata for block 15000001"

### üîÑ Next Steps
Need to resolve the blessed data limitation by either:
1. Adjusting the test to use only the available block range (15000000 to 15000000)
2. Finding/creating a dataset with more blessed data blocks
3. Using a different approach for testing with limited data

## Implementation Details

```rust
#[tokio::test]
async fn sql_dataset_input_batch_size() {
    let test_env = TestEnv::temp("sql_dataset_input_batch_size").await.unwrap();
    
    // Restore dependency dataset
    restore_blessed_dataset("eth_firehose", &test_env.metadata_db)
        .await
        .unwrap();
    
    // Execute dump with input_batch_size_blocks=1
    let dataset_name = "sql_over_eth_firehose";
    let start_block = 15_000_000;
    let end_block = 15_000_003;
    
    dump_dataset_with_batch_size(
        test_env.config.clone(),
        dataset_name,
        start_block,
        end_block,
        1,
        1, // input_batch_size_blocks=1
    )
    .await
    .unwrap();
    
    // Get catalog and count files
    let catalog = catalog_for_dataset(
        dataset_name,
        &test_env.dataset_store,
        test_env.metadata_db.clone(),
    )
    .await
    .unwrap();
    
    let table = catalog
        .tables()
        .iter()
        .find(|t| t.table_name() == "even_blocks")
        .expect("even_blocks table not found");
    
    let files = table.files().await.unwrap();
    let file_count = files.len();
    
    // Should have 4 files with batch size 1 for 4 blocks
    assert_eq!(file_count, 4, "Expected 4 files with batch size 1 for 4 blocks");
}
```

## Helper Function Added
```rust
pub(crate) async fn dump_dataset_with_batch_size(
    config: Arc<Config>,
    dataset_name: &str,
    start: u64,
    end: u64,
    n_jobs: u16,
    input_batch_size_blocks: u64,
) -> Result<(), BoxError>
```
