use std::{any::Any, borrow::Cow, collections::BTreeMap, path::PathBuf, sync::Arc};

use datafusion::{
    arrow::{compute::SortOptions, datatypes::SchemaRef, error::ArrowError},
    catalog::{Session, TableProvider},
    common::{
        project_schema, stats::Precision, Column as LogicalColumn, ColumnStatistics, Constraints,
        Statistics, ToDFSchema,
    },
    datasource::{
        file_format::{parquet::ParquetFormat, FileFormat},
        listing::{ListingTable, ListingTableUrl, PartitionedFile},
        physical_plan::FileScanConfig,
        TableType,
    },
    error::{DataFusionError, Result as DataFusionResult},
    execution::{object_store::ObjectStoreUrl, SessionState},
    logical_expr::{
        utils::conjunction, Expr as LogicalExpr, LogicalPlan, SortExpr, TableProviderFilterPushDown,
    },
    parquet::{
        arrow::async_reader::{AsyncFileReader, ParquetObjectReader},
        file::metadata::{KeyValue, ParquetMetaData},
    },
    physical_expr::{create_physical_expr, LexOrdering, PhysicalSortExpr},
    physical_plan::{empty::EmptyExec, expressions::Column, ExecutionPlan},
    sql::TableReference,
};
use futures::{
    future::BoxFuture,
    stream::{self, BoxStream},
    FutureExt, StreamExt, TryStreamExt,
};
use metadata_db::{FileMetadata as NozzleFileMetadata, LocationId, MetadataDb, TableId};
use object_store::{
    path::{Error as PathError, Path},
    ObjectMeta, ObjectStore,
};
use url::Url;

use crate::{
    catalog::logical::Table as LogicalTable,
    meta_tables::scanned_ranges::{ScannedRange, METADATA_KEY},
    BoxError, Store, BLOCK_NUM,
};

/// A table in the Nozzle catalog.
///
/// ## Variants
/// - [Local](PhysicalTable::Local): A table that is backed by files generated by Nozzle dump.
/// - [Optimized](PhysicalTable::Optimized): A table that is backed by files generated by Nozzle dump,
///  optimized with an external Metadata provider.
/// - [Other](PhysicalTable::Other): A table that is not backed by an object store but may be present
/// in a Nozzle dataset.
#[derive(Clone, Debug)]
pub enum PhysicalTable {
    /// A [`DumpListingTable`] that is backed by files generated by Nozzle dump.
    Local {
        /// The [`DumpListingTable`] that is backed by files generated by Nozzle dump.
        dump: Arc<DumpListingTable>,
        /// The [`ObjectStore`] that is used to access the files.
        object_store: Arc<dyn ObjectStore>,
    },
    /// A [`DumpListingTable`] that is backed by files generated by Nozzle dump,
    /// This table is optimized with an external Metadata provider
    Optimized {
        /// The [`DumpListingTable`] that is backed by files generated by Nozzle dump.
        dump: Arc<DumpListingTable>,
        /// The [`ObjectStore`] that is used to access the files.
        object_store: Arc<dyn ObjectStore>,
        /// The [`MetadataDb`] that is used to optimize the table.
        metadata_provider: Arc<MetadataDb>,
        /// The location ID of the table in the metadata provider.
        location_id: LocationId,
    },
    /// For memory tables, streaming tables, or other tables that are
    /// not backed by an object store but may be present in a nozzle
    /// dataset. These may be used for cached statistics or metadata
    /// providers for dump tables.
    Other {
        /// The table provider that is not backed by an object store.
        table_provider: Arc<dyn TableProvider>,
        /// The logical table that is not backed by an object store.
        logical_table: Arc<LogicalTable>,
        /// The table reference for the table that is not backed by an object store.
        table_ref: Arc<TableReference>,
    },
}

/// Methods for creating a [`PhysicalTable`]
impl PhysicalTable {
    pub fn try_at_active_location(
        dataset_name: &str,
        logical_table: &LogicalTable,
        url: Url,
        location_id: LocationId,
        object_store: Arc<dyn ObjectStore>,
        metadata_provider: impl AsRef<MetadataDb>,
    ) -> Result<Self, BoxError> {
        let metadata_provider = Arc::new(metadata_provider.as_ref().clone());
        let dataset_version = url
            .path_segments()
            .ok_or(PathError::InvalidPath {
                path: PathBuf::from(url.path()),
            })
            .map(|mut s| {
                s.nth_back(2)
                    .filter(|segment| *segment != dataset_name)
                    .map(ToString::to_string)
            })?;
        let dump = DumpListingTable::new(dataset_name, dataset_version, logical_table, url)?;

        let table = Self::Optimized {
            dump: Arc::new(dump),
            object_store: object_store.clone(),
            metadata_provider: metadata_provider.clone(),
            location_id,
        };

        Ok(table)
    }

    pub async fn try_next_revision(
        logical_table: &LogicalTable,
        data_store: &Store,
        dataset_name: &str,
        metadata_provider: impl AsRef<MetadataDb>,
    ) -> Result<Self, BoxError> {
        let metadata_provider = Arc::new(metadata_provider.as_ref().clone());

        let table_id = TableId {
            dataset: dataset_name,
            dataset_version: None,
            table: &logical_table.name,
        };

        let path = make_location_path(table_id);
        let url = data_store.url().join(&path)?;
        let location_id = metadata_provider
            .register_location(table_id, data_store.bucket(), &path, &url, false)
            .await?;
        metadata_provider
            .set_active_location(table_id, &url.as_str())
            .await?;

        let object_store = data_store.object_store();

        Self::try_at_active_location(
            dataset_name,
            logical_table,
            url,
            location_id,
            object_store,
            metadata_provider,
        )
    }

    pub(super) fn try_at_static_location(
        store: Arc<Store>,
        dataset_name: &str,
        logical_table: LogicalTable,
    ) -> Result<Self, BoxError> {
        validate_name(&logical_table.name)?;
        let dataset_version = None;
        let table_id = TableId {
            dataset: dataset_name,
            dataset_version,
            table: &logical_table.name,
        };
        let input = format!("{}/{}/", dataset_name, table_id.table);
        let url = store.url().join(&input)?;

        let dump = DumpListingTable::new(dataset_name, None, &logical_table, url)?;

        let table = Self::Local {
            dump: Arc::new(dump),
            object_store: store.object_store(),
        };

        Ok(table)
    }
}

impl PhysicalTable {
    pub fn logical_table(&self) -> LogicalTable {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.logical_table(),
            Other { logical_table, .. } => logical_table.as_ref().clone(),
        }
    }
    pub fn table_ref(&self) -> Arc<TableReference> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.table_ref(),
            Other { table_ref, .. } => table_ref.clone(),
        }
    }

    pub fn catalog_schema(&self) -> String {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.dataset.clone(),
            Other { table_ref, .. } => table_ref.schema().unwrap().to_string(),
        }
    }

    pub fn network(&self) -> Option<String> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.network.clone(),
            Other { logical_table, .. } => logical_table.network.clone(),
        }
    }

    pub fn table_id(&self) -> TableId<'_> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.table_id(),
            Other { table_ref, .. } => TableId {
                dataset: table_ref.schema().unwrap(),
                dataset_version: None,
                table: table_ref.table(),
            },
        }
    }

    pub fn table_name(&self) -> &str {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.table_name(),
            Other { table_ref, .. } => table_ref.table(),
        }
    }

    pub fn url(&self) -> Option<Url> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => Some(dump.url().clone()),
            Other { table_provider, .. } => table_provider
                .as_any()
                .downcast_ref::<ListingTable>()
                .map(|t| {
                    t.table_paths()
                        .first()
                        .map(|url| Url::parse(url.as_str()).ok())
                        .unwrap()
                })
                .flatten(),
        }
    }

    pub fn path(&self) -> DataFusionResult<Path> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => Ok(dump.path().clone()),
            Other {
                table_provider,
                table_ref,
                ..
            } => {
                let listing_table = table_provider
                    .as_any()
                    .downcast_ref::<ListingTable>()
                    .ok_or(DataFusionError::Internal(format!(
                        "Failed to downcast to ListingTable for table: {table_ref:?}"
                    )))?;
                let path = listing_table
                    .table_paths()
                    .first()
                    .ok_or(DataFusionError::Internal(format!(
                        "ListingTable {table_ref} "
                    )))?;

                Ok(Path::from_url_path(path)?)
            }
        }
    }

    pub fn object_store(&self) -> DataFusionResult<Arc<dyn ObjectStore>> {
        use PhysicalTable::*;
        match self {
            Local { object_store, .. } | Optimized { object_store, .. } => Ok(object_store.clone()),
            Other { .. } => Err(DataFusionError::Internal(
                "PhysicalTable::object_store: Other table not supported".to_string(),
            )),
        }
    }

    pub fn is_meta(&self) -> bool {
        use PhysicalTable::*;
        match self {
            Local { .. } | Optimized { .. } => false,
            Other { logical_table, .. } => logical_table.is_meta(),
        }
    }

    pub fn location_id(&self) -> Option<LocationId> {
        use PhysicalTable::*;
        match self {
            Optimized { location_id, .. } => Some(*location_id),
            _ => None,
        }
    }

    pub fn order_exprs(&self) -> Vec<Vec<SortExpr>> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => {
                let physical_order_exprs = dump.order_exprs.clone();

                physical_order_exprs
                    .iter()
                    .map(|sort_exprs| {
                        sort_exprs
                            .iter()
                            .map(
                                |PhysicalSortExpr {
                                     expr,
                                     options:
                                         SortOptions {
                                             descending,
                                             nulls_first,
                                         },
                                 }| {
                                    let expr = expr
                                        .as_any()
                                        .downcast_ref::<Column>()
                                        .cloned()
                                        .map(|col| {
                                            LogicalExpr::Column(LogicalColumn::new(
                                                Some(self.table_ref().as_ref().clone()),
                                                col.name(),
                                            ))
                                        })
                                        .expect("PhysicalSortExpr::expr should be a Column");

                                    SortExpr::new(expr, !*descending, *nulls_first)
                                },
                            )
                            .collect()
                    })
                    .collect()
            }
            _ => unreachable!(),
        }
    }

    pub fn physical_sort_exprs(&self) -> Vec<Vec<PhysicalSortExpr>> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.order_exprs.clone(),
            _ => unreachable!(),
        }
    }

    pub fn ranges<'a>(
        &'a self,
    ) -> DataFusionResult<BoxFuture<'a, DataFusionResult<Vec<(u64, u64)>>>> {
        Ok(self.stream_scanned_ranges()?.try_collect().boxed())
    }

    pub fn parquet_files<'a>(
        &'a self,
    ) -> DataFusionResult<BoxFuture<'a, DataFusionResult<BTreeMap<String, ObjectMeta>>>> {
        Ok(self
            .stream_files()?
            .try_collect::<BTreeMap<String, ObjectMeta>>()
            .boxed())
    }
}

/// Streaming methods for the [`PhysicalTable`].
impl PhysicalTable {
    fn stream_scanned_ranges<'a>(
        &'a self,
    ) -> DataFusionResult<BoxStream<'a, DataFusionResult<(u64, u64)>>> {
        use PhysicalTable::*;
        match self {
            Local { dump, object_store } => Ok(dump.ranges_stream(object_store)),
            Optimized {
                dump,
                metadata_provider,
                ..
            } => {
                let tbl = dump.table_id();
                Ok(metadata_provider
                    .stream_ranges(tbl)
                    .map_err(|e| DataFusionError::External(Box::new(e)))
                    .map_ok(|(start, end)| (start as u64, end as u64))
                    .boxed())
            }
            Other { .. } => unimplemented!("stream_scanned_ranges not implemented for Other table"),
        }
    }

    fn stream_files<'a>(
        &'a self,
    ) -> DataFusionResult<BoxStream<'a, DataFusionResult<(String, ObjectMeta)>>> {
        use PhysicalTable::*;
        match self {
            Local { dump, object_store } => Ok(dump.stream_files(object_store)),
            Optimized {
                dump,
                metadata_provider,
                ..
            } => {
                let tbl = dump.table_id();
                Ok(metadata_provider
                    .stream_object_meta(tbl)
                    .map_err(|e| DataFusionError::External(Box::new(e)))
                    .boxed())
            }
            _ => unimplemented!(),
        }
    }

    pub fn stream_object_readers<'a>(
        &'a self,
    ) -> DataFusionResult<BoxStream<'a, DataFusionResult<(ParquetObjectReader, ObjectMeta)>>> {
        use PhysicalTable::*;
        match self {
            Local { dump, object_store } => Ok(dump.object_reader_stream(object_store).boxed()),
            Optimized { .. } => Err(DataFusionError::Internal(
                "PhysicalTable::stream_object_readers: Optimized table not supported".to_string(),
            )),
            _ => unimplemented!(),
        }
    }

    fn stream_nozzle_metadata<'a>(
        &'a self,
    ) -> DataFusionResult<BoxStream<'a, DataFusionResult<NozzleFileMetadata>>> {
        use PhysicalTable::*;
        match self {
            Local {
                dump, object_store, ..
            } => Ok(dump.nozzle_metadata_stream(object_store)),
            Optimized {
                dump,
                metadata_provider,
                ..
            } => Ok(metadata_provider
                .stream_nozzle_metadata(dump.table_id())
                .map_err(|e| DataFusionError::External(Box::new(e)))
                .boxed()),
            Other { .. } => Err(DataFusionError::Internal(
                "PhysicalTable::stream_nozzle_metadata: Other table not supported".to_string(),
            )),
        }
    }

    async fn list_files_for_scan<'a>(
        &'a self,
        _filters: &'a [LogicalExpr],
        _limit: Option<usize>,
    ) -> DataFusionResult<(Vec<Vec<PartitionedFile>>, Statistics)> {
        use Precision::*;

        let mut files = vec![];
        let mut table_stats = Statistics::new_unknown(self.schema().as_ref());
        let mut stream = self.stream_nozzle_metadata()?;

        let block_num_idx = self.schema().index_of(BLOCK_NUM)?;

        while let Some(NozzleFileMetadata {
            object_meta,
            range: (range_start, range_end),
            row_count,
            data_size,
            size_hint,
        }) = stream.try_next().await?
        {
            let block_num_stats = ColumnStatistics {
                null_count: Exact(0),
                max_value: Inexact(range_end.into()),
                min_value: Inexact(range_start.into()),
                ..Default::default()
            };

            let mut file_stats = Statistics::new_unknown(&self.schema().clone());

            file_stats
                .column_statistics
                .insert(block_num_idx, block_num_stats);

            if let Some(num_rows) = row_count {
                file_stats.num_rows = Exact(num_rows as usize);
            }

            if let Some(total_size_bytes) = data_size {
                file_stats.total_byte_size = Exact(total_size_bytes as usize);
            }

            let mut partitioned_file = PartitionedFile::from(object_meta);
            partitioned_file.metadata_size_hint = size_hint.map(|s| s as usize);

            update_table_stats(&mut table_stats, &file_stats, &[block_num_idx])?;

            files.push(partitioned_file);
        }

        let split_files = split_files(files, 10);

        Ok((split_files, table_stats))
    }
}

impl TryFrom<Arc<dyn TableProvider>> for PhysicalTable {
    type Error = DataFusionError;
    fn try_from(dyn_table: Arc<dyn TableProvider>) -> Result<Self, Self::Error> {
        if let Some(table) = dyn_table.as_ref().as_any().downcast_ref::<PhysicalTable>() {
            return Ok(table.clone());
        } else {
            return Err(DataFusionError::Internal(format!(
                "Failed to downcast to Table for table: {:?}",
                dyn_table.type_id()
            )));
        }
    }
}

impl
    TryFrom<(
        Arc<dyn TableProvider>,
        Option<Arc<LogicalTable>>,
        Option<Arc<TableReference>>,
    )> for PhysicalTable
{
    type Error = DataFusionError;
    fn try_from(
        (table_provider, logical_table, table_ref): (
            Arc<dyn TableProvider>,
            Option<Arc<LogicalTable>>,
            Option<Arc<TableReference>>,
        ),
    ) -> Result<Self, Self::Error> {
        if let Ok(table) = PhysicalTable::try_from(table_provider.clone()) {
            return Ok(table);
        } else if let (Some(logical_table), Some(table_ref)) = (logical_table, table_ref) {
            return Ok(PhysicalTable::Other {
                table_provider,
                logical_table,
                table_ref,
            });
        } else {
            return Err(DataFusionError::Internal(format!(
                "Failed to downcast to Table for table: {:?}",
                table_provider.type_id()
            )));
        }
    }
}

#[async_trait::async_trait]
impl TableProvider for PhysicalTable {
    fn as_any(&self) -> &dyn Any {
        self
    }

    fn schema(&self) -> SchemaRef {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.schema.clone(),
            Other { table_provider, .. } => table_provider.schema(),
        }
    }

    fn table_type(&self) -> TableType {
        use PhysicalTable::*;
        match self {
            Other { table_provider, .. } => table_provider.table_type(),
            _ => TableType::Base,
        }
    }

    // Not using `async_trait` here as this is a wrapper for the
    // `scan` method of the underlying table provider.
    async fn scan(
        &self,
        state: &dyn Session,
        projection: Option<&Vec<usize>>,
        filters: &[LogicalExpr],
        limit: Option<usize>,
    ) -> DataFusionResult<Arc<dyn ExecutionPlan>> {
        use PhysicalTable::*;
        match self {
            Other { table_provider, .. } => {
                table_provider.scan(state, projection, filters, limit).await
            }
            Local { dump, .. } | Optimized { dump, .. } => {
                dump.scan(
                    state,
                    projection,
                    filters,
                    limit,
                    self.list_files_for_scan(filters, limit).boxed(),
                )
                .await
            }
        }
    }

    fn get_table_definition(&self) -> Option<&str> {
        use PhysicalTable::*;
        match self {
            Other { table_provider, .. } => table_provider.get_table_definition(),
            _ => None,
        }
    }

    fn constraints(&self) -> Option<&Constraints> {
        use PhysicalTable::*;
        match self {
            Other { table_provider, .. } => table_provider.constraints(),
            _ => None,
        }
    }

    fn get_column_default(&self, _column: &str) -> Option<&LogicalExpr> {
        use PhysicalTable::*;
        match self {
            Other { table_provider, .. } => table_provider.get_column_default(_column),
            _ => None,
        }
    }

    fn get_logical_plan(&self) -> Option<Cow<LogicalPlan>> {
        use PhysicalTable::*;
        match self {
            Other { table_provider, .. } => table_provider.get_logical_plan(),
            _ => None,
        }
    }

    fn statistics(&self) -> Option<Statistics> {
        use PhysicalTable::*;
        match self {
            Local { .. } => None,
            Optimized { .. } => None,
            Other { table_provider, .. } => table_provider.statistics(),
        }
    }

    fn supports_filters_pushdown(
        &self,
        filters: &[&LogicalExpr],
    ) -> DataFusionResult<Vec<datafusion::logical_expr::TableProviderFilterPushDown>> {
        use PhysicalTable::*;
        match self {
            Local { .. } | Optimized { .. } => Ok(vec![
                TableProviderFilterPushDown::Unsupported;
                filters.len()
            ]),
            Other { table_provider, .. } => table_provider.supports_filters_pushdown(filters),
        }
    }
}

#[derive(Clone, Debug)]
pub struct DumpListingTable {
    name: String,
    dataset: String,
    dataset_version: Option<String>,
    network: Option<String>,

    url: Url,
    path: Path,

    format: Arc<ParquetFormat>,
    schema: SchemaRef,
    order_exprs: Vec<Vec<PhysicalSortExpr>>,
}

impl DumpListingTable {
    pub fn new(
        dataset_name: impl Into<String>,
        dataset_version: Option<String>,
        logical_table: &LogicalTable,
        url: Url,
    ) -> Result<Self, BoxError> {
        let dataset = dataset_name.into();
        let name = logical_table.name.clone();
        let network = logical_table.network.clone();
        let path = Path::from_url_path(url.path())?;

        let format = Arc::new(ParquetFormat::default());
        let schema = logical_table.schema.clone();
        let sort_exprs =
            logical_table
                .sorted_by()
                .into_iter()
                .try_fold(vec![], |mut acc, name| {
                    let index = schema.index_of(&name)?;
                    let col = Column::new(&name, index);
                    let sort_options = SortOptions {
                        descending: false,
                        nulls_first: false,
                    };
                    let sort_expr = PhysicalSortExpr::new(Arc::new(col), sort_options);
                    acc.push(sort_expr);
                    Ok::<_, ArrowError>(acc)
                })?;

        let order_exprs = vec![sort_exprs];

        Ok(Self {
            name,
            dataset,
            dataset_version,
            network,
            url,
            path,
            format,
            schema,
            order_exprs,
        })
    }

    fn logical_table(&self) -> LogicalTable {
        LogicalTable {
            name: self.name.clone(),
            schema: self.schema.clone(),
            network: self.network.clone(),
        }
    }

    /// Returns the name of the table.
    fn table_name(&self) -> &str {
        &self.name
    }

    /// Returns the table_reference for this table.
    ///
    /// Returns a TableReference::Partial where:
    /// - schema is the dataset name with version if it exists
    /// - table is the table name
    ///
    /// \<dataset>.\<table>
    ///
    /// TODO: The dataset_version should be a partition specifier, not a schema
    /// identifier.
    fn table_ref(&self) -> Arc<TableReference> {
        let schema = self.dataset.as_str();
        let table = self.name.as_str();
        Arc::new(TableReference::partial(schema, table))
    }

    fn table_id<'a>(&'a self) -> TableId<'a> {
        TableId::<'a> {
            dataset: &self.dataset,
            dataset_version: self.dataset_version.as_ref().map(String::as_str),
            table: &self.name,
        }
    }

    fn url(&self) -> &Url {
        &self.url
    }

    fn path(&self) -> &Path {
        &self.path
    }

    fn prefix(&self) -> Option<&Path> {
        Some(self.path())
    }

    pub fn listing_table_url(&self) -> DataFusionResult<ListingTableUrl> {
        ListingTableUrl::parse(self.url())
    }

    pub fn object_store_url(&self) -> DataFusionResult<ObjectStoreUrl> {
        let object_store_url = self.url().as_str().split("/").take(1).collect::<String>() + "//";
        ObjectStoreUrl::parse(object_store_url)
    }

    pub fn object_store(
        &self,
        session_state: &SessionState,
    ) -> DataFusionResult<Arc<dyn ObjectStore>> {
        let url = self.object_store_url()?;
        let object_store = session_state.runtime_env().object_store(url)?;
        Ok(object_store)
    }
}

/// Streaming methods for the [`DumpListingTable`]
impl DumpListingTable {
    fn stream_files<'a>(
        &'a self,
        object_store: &'a dyn ObjectStore,
    ) -> BoxStream<'a, DataFusionResult<(String, ObjectMeta)>> {
        object_store
            .list(self.prefix())
            .map(|entry| {
                let meta = entry?;
                let file_name = meta
                    .location
                    .filename()
                    .ok_or(PathError::InvalidPath {
                        path: PathBuf::from(meta.location.to_string()),
                    })?
                    .to_string();

                Ok((file_name, meta))
            })
            .boxed()
    }

    fn object_reader_stream<'a>(
        &'a self,
        store: &'a Arc<dyn ObjectStore>,
    ) -> BoxStream<'a, DataFusionResult<(ParquetObjectReader, ObjectMeta)>> {
        self.stream_files(store)
            .zip(stream::repeat(store))
            .map(|(res, store)| {
                let (_, meta) = res?;
                let object_reader = ParquetObjectReader::new(store.clone(), meta.clone());
                Ok((object_reader, meta))
            })
            .boxed()
    }

    fn parquet_metadata_stream<'a>(
        &'a self,
        object_store: &'a Arc<dyn ObjectStore>,
    ) -> BoxStream<'a, DataFusionResult<(Arc<ParquetMetaData>, ObjectMeta)>> {
        self.object_reader_stream(object_store)
            .try_filter_map(|(mut reader, meta)| async move {
                let metadata = reader.get_metadata().await?;
                Ok(Some((metadata, meta)))
            })
            .boxed()
    }

    fn ranges_stream<'a>(
        &'a self,
        object_store: &'a Arc<dyn ObjectStore>,
    ) -> BoxStream<'a, DataFusionResult<(u64, u64)>> {
        self.nozzle_metadata_stream(object_store)
            .map(|res| {
                let nozzle_metadata = res?;
                let range = (
                    nozzle_metadata.range.0 as u64,
                    nozzle_metadata.range.1 as u64,
                );

                Ok(range)
            })
            .boxed()
    }

    fn nozzle_metadata_stream<'a>(
        &'a self,
        object_store: &'a Arc<dyn ObjectStore>,
    ) -> BoxStream<'a, DataFusionResult<NozzleFileMetadata>> {
        self.parquet_metadata_stream(object_store)
            .map(|res| {
                let (parquet_meta, object_meta) = res?;
                let file_metadata = parquet_meta.file_metadata();

                let kv_metadata =
                    file_metadata
                        .key_value_metadata()
                        .ok_or(DataFusionError::Internal(format!(
                            "Key-value metadata not found in parquet file metadata for file: {}",
                            object_meta.location
                        )))?;

                let nozzle_metadata_key_value_pair = kv_metadata
                    .into_iter()
                    .find(|KeyValue { key, .. }| key == METADATA_KEY)
                    .ok_or(crate::ArrowError::ParquetError(format!(
                        "Missing key: {} in file metadata for file {}",
                        METADATA_KEY, object_meta.location
                    )))?;

                let scanned_range_json = nozzle_metadata_key_value_pair.value.as_ref().ok_or(
                    crate::ArrowError::ParseError(format!(
                        "Missing value for key: {} in file metadata for file {}",
                        METADATA_KEY, object_meta.location
                    )),
                )?;

                let scanned_range: ScannedRange = serde_json::from_str(scanned_range_json)
                    .map_err(|e| {
                        DataFusionError::Internal(format!(
                            "Failed to parse JSON from value for key: {} in file metadata for file {}: {}",
                            METADATA_KEY,
                            object_meta.location,
                            e
                        ))
                    })?;

                let file_metadata = NozzleFileMetadata {
                    object_meta,
                    range: (
                        scanned_range.range_start as i64,
                        scanned_range.range_end as i64,
                    ),
                    row_count: None,
                    data_size: None,
                    size_hint: None,
                };

                Ok(file_metadata)
            })
            .boxed()
    }

    async fn scan<'a>(
        &self,
        state: &dyn Session,
        projection: Option<&Vec<usize>>,
        filters: &[LogicalExpr],
        limit: Option<usize>,
        files_and_stats_fut: BoxFuture<
            'a,
            DataFusionResult<(Vec<Vec<PartitionedFile>>, Statistics)>,
        >,
    ) -> DataFusionResult<Arc<dyn ExecutionPlan>> {
        let (mut partitioned_file_lists, statistics) = files_and_stats_fut.await?;

        let session_state = state.as_any().downcast_ref::<SessionState>().unwrap();
        // if no files need to be read, return an `EmptyExec`
        if partitioned_file_lists.is_empty() {
            let projected_schema = project_schema(&self.schema, projection)?;
            return Ok(Arc::new(EmptyExec::new(projected_schema)));
        }

        let mut output_ordering: Vec<LexOrdering> = vec![];

        for sort_exprs in self.order_exprs.iter() {
            let mut lex_ordering = LexOrdering::default();
            for sort_expr in sort_exprs {
                lex_ordering.push(sort_expr.clone());
            }
            output_ordering.push(lex_ordering);
        }

        match state
            .config_options()
            .execution
            .split_file_groups_by_statistics
            .then(|| {
                output_ordering.first().map(|output_ordering| {
                    FileScanConfig::split_groups_by_statistics(
                        &self.schema,
                        &partitioned_file_lists,
                        output_ordering,
                    )
                })
            })
            .flatten()
        {
            Some(Err(e)) => log::debug!("failed to split file groups by statistics: {e}"),
            Some(Ok(new_groups)) => {
                if new_groups.len() <= 10 {
                    partitioned_file_lists = new_groups;
                } else {
                    log::debug!("attempted to split file groups by statistics, but there were more file groups than target_partitions; falling back to unordered")
                }
            }
            None => {} // no ordering required
        };

        let filters = match conjunction(filters.to_vec()) {
            Some(expr) => {
                let table_df_schema = self.schema.as_ref().clone().to_dfschema()?;
                let filters =
                    create_physical_expr(&expr, &table_df_schema, state.execution_props())?;
                Some(filters)
            }
            None => None,
        };

        let file_schema = self.schema.clone();
        let object_store_url = self.object_store_url()?;

        self.format
            .create_physical_plan(
                session_state,
                FileScanConfig::new(object_store_url, file_schema)
                    .with_file_groups(partitioned_file_lists)
                    .with_statistics(statistics)
                    .with_projection(projection.cloned())
                    .with_limit(limit)
                    .with_output_ordering(output_ordering),
                filters.as_ref(),
            )
            .await
    }
}

pub(super) fn validate_name(name: &str) -> Result<(), BoxError> {
    if let Some(c) = name
        .chars()
        .find(|&c| !(c.is_ascii_lowercase() || c == '_' || c.is_numeric()))
    {
        return Err(format!(
            "names must be lowercase and contain only letters, underscores, and numbers, \
             the name: '{name}' is not allowed because it contains the character '{c}'"
        )
        .into());
    }

    Ok(())
}

fn split_files(mut files: Vec<PartitionedFile>, n: usize) -> Vec<Vec<PartitionedFile>> {
    if files.is_empty() {
        return vec![];
    }
    files.sort_by(|a, b| a.path().cmp(b.path()));

    let chunk_size = files.len().div_ceil(n);

    let mut chunks = Vec::with_capacity(n);

    let mut current_chunk = Vec::with_capacity(chunk_size);

    for file in files.drain(..) {
        current_chunk.push(file);
        if current_chunk.len() == chunk_size {
            let full_chunk = std::mem::replace(&mut current_chunk, Vec::with_capacity(chunk_size));
            chunks.push(full_chunk);
        }
    }

    if !current_chunk.is_empty() {
        chunks.push(current_chunk)
    }

    chunks
}

fn update_table_stats(
    table_stats: &mut Statistics,
    file_stats: &Statistics,
    columns_to_update: &[usize],
) -> DataFusionResult<()> {
    for index in columns_to_update {
        if let (Some(table_stats), Some(file_stats)) = (
            table_stats.column_statistics.get_mut(*index),
            file_stats.column_statistics.get(*index),
        ) {
            table_stats.max_value = table_stats.max_value.max(&file_stats.max_value);
            table_stats.min_value = table_stats.min_value.min(&file_stats.min_value);
            table_stats.null_count = table_stats.null_count.add(&file_stats.null_count);
            table_stats.distinct_count = table_stats.distinct_count.add(&file_stats.distinct_count);
            table_stats.sum_value = table_stats.sum_value.add(&file_stats.sum_value);
        } else {
            return Err(DataFusionError::Execution(
                format!("Column statistics not found for index: {index}").into(),
            ));
        }
    }
    table_stats.num_rows = table_stats.num_rows.add(&file_stats.num_rows);
    table_stats.total_byte_size = table_stats.total_byte_size.add(&file_stats.total_byte_size);

    Ok(())
}

// The path format is: `<dataset>/[<version>/]<table>/<UUIDv7>/`
pub fn make_location_path(table_id: TableId<'_>) -> String {
    let mut path = String::new();
    // Add dataset
    path.push_str(table_id.dataset);
    path.push('/');

    // Add version if present
    if let Some(version) = table_id.dataset_version {
        path.push_str(version);
        path.push('/');
    }

    // Add table
    path.push_str(table_id.table);
    path.push('/');

    // Add UUIDv7
    let uuid = uuid::Uuid::now_v7();
    path.push_str(&uuid.to_string());
    path.push('/');

    path
}
