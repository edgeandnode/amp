{
  "opencli": "1.0.0",
  "info": {
    "title": "Nozzle CLI",
    "description": "High-performance ETL (Extract, Transform, Load) architecture for blockchain data services. Extracts blockchain data from various sources, transforms it via SQL queries, and serves it through multiple query interfaces.",
    "version": "0.1.0-20251005",
    "contact": {
      "name": "Edge & Node",
      "url": "https://github.com/edgeandnode/project-nozzle"
    }
  },
  "environment": [
    {
      "name": "NOZZLE_CONFIG",
      "description": "The configuration file to use. This file defines where to look for dataset definitions and providers, along with many other configuration options. This argument is optional for the generate-manifest command."
    }
  ],
  "commands": {
    "migrate": {
      "summary": "Run migrations on the metadata database",
      "description": "Executes database schema migrations on the PostgreSQL metadata database. This command requires the --config parameter to be set, which should point to a configuration file containing the metadata database connection details. Migrations are applied automatically and cannot be rolled back through this command.",
      "parameters": [],
      "responses": {
        "0": {
          "description": "Migrations completed successfully"
        },
        "1": {
          "description": "Error occurred during migration or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "nozzle --config config.toml migrate",
          "description": "Run migrations using the specified configuration file"
        }
      ]
    },
    "worker": {
      "summary": "Run distributed worker node",
      "description": "Starts a worker node that connects to the metadata database and processes distributed dump tasks. The worker polls for available jobs and executes them. Multiple workers can run in parallel to process different portions of data extraction jobs. The node-id must be unique across all running workers. Workers require access to the metadata database configured in the config file and run continuously until terminated.",
      "parameters": [
        {
          "name": "node-id",
          "in": "option",
          "description": "The unique identifier for this worker node. Used to track worker status and job assignments in the distributed system.",
          "required": true,
          "schema": {
            "type": "string"
          },
          "x-environment": "NOZZLE_NODE_ID"
        }
      ],
      "responses": {
        "0": {
          "description": "Worker shut down gracefully"
        },
        "1": {
          "description": "Error occurred during worker operation or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "nozzle --config config.toml worker --node-id worker-01",
          "description": "Start a worker node with ID 'worker-01'"
        },
        {
          "command": "NOZZLE_CONFIG=config.toml NOZZLE_NODE_ID=worker-02 nozzle worker",
          "description": "Start a worker using environment variables"
        }
      ]
    },
    "generate-manifest": {
      "summary": "Generate dataset manifests",
      "description": "Generates dataset manifest files for different dataset types (firehose, substreams, evm-rpc, etc.). The manifest can be output to a file, directory, or stdout. For Substreams datasets, additional manifest and module parameters are required. The --config parameter is NOT required for this command (unlike other commands). If --out is omitted, the manifest is printed to stdout for easy piping. If --out points to a directory, the output file will be named after the 'kind' parameter. The generated manifest is in JSON format and can be used with the dump command.",
      "parameters": [
        {
          "name": "network",
          "in": "option",
          "description": "The name of the network for which the dataset manifest is being generated (e.g., mainnet, sepolia, polygon).",
          "required": true,
          "schema": {
            "type": "string"
          },
          "x-environment": "GM_NETWORK"
        },
        {
          "name": "kind",
          "in": "option",
          "description": "The kind/type of the dataset (e.g., firehose, substreams, evm-rpc). This determines the dataset extractor type and structure.",
          "required": true,
          "schema": {
            "type": "string"
          },
          "x-environment": "GM_KIND"
        },
        {
          "name": "name",
          "in": "option",
          "description": "The name of the dataset. This will be used as the dataset identifier in the system.",
          "required": true,
          "schema": {
            "type": "string"
          },
          "x-environment": "GM_NAME"
        },
        {
          "name": "out",
          "in": "option",
          "alias": ["o"],
          "description": "Output file or directory path. If a directory is specified, the file will be named '{kind}.json'. If not specified, the manifest will be printed to stdout.",
          "required": false,
          "schema": {
            "type": "string",
            "format": "path"
          },
          "x-environment": "GM_OUT"
        },
        {
          "name": "manifest",
          "in": "option",
          "description": "Substreams package manifest URL. This option is required when generating manifests for DatasetKind::Substreams.",
          "required": false,
          "schema": {
            "type": "string",
            "format": "uri"
          },
          "x-environment": "GM_SS_MANIFEST_URL"
        },
        {
          "name": "module",
          "in": "option",
          "description": "Substreams output module name. This option is required when generating manifests for DatasetKind::Substreams, specifying which module's output to use.",
          "required": false,
          "schema": {
            "type": "string"
          },
          "x-environment": "GM_SS_MODULE"
        }
      ],
      "responses": {
        "0": {
          "description": "Manifest generated successfully"
        },
        "1": {
          "description": "Error occurred during manifest generation or invalid parameters"
        }
      },
      "x-examples": [
        {
          "command": "nozzle generate-manifest --network mainnet --kind firehose --name eth_firehose",
          "description": "Generate a firehose dataset manifest and output to stdout"
        },
        {
          "command": "nozzle generate-manifest --network mainnet --kind evm-rpc --name eth_rpc -o manifests/",
          "description": "Generate an EVM RPC dataset manifest and save to manifests/evm-rpc.json"
        },
        {
          "command": "nozzle generate-manifest --network mainnet --kind substreams --name my_substream --manifest https://example.com/substreams.spkg --module map_events -o output.json",
          "description": "Generate a Substreams dataset manifest with required Substreams parameters"
        }
      ]
    },
    "dev": {
      "summary": "Start development server with embedded worker",
      "description": "Starts query servers with an embedded worker in the same process for simplified local development. This is the recommended mode for local testing and development, eliminating the need to run a separate worker process. By default, all three servers (Arrow Flight, JSON Lines, and Admin API) are started. Servers can be selectively enabled using flags. If no server flags are specified, ALL THREE servers are enabled by default. When any server flag is specified, only the explicitly enabled servers will start. Default ports: Arrow Flight (1602), JSON Lines (1603), Admin API (1610). The server and embedded worker run continuously until terminated (Ctrl+C or kill signal).",
      "parameters": [
        {
          "name": "flight-server",
          "in": "flag",
          "description": "Enable Arrow Flight RPC Server. This provides a high-performance binary protocol for querying data (default port 1602). Uses Apache Arrow Flight for efficient data transfer.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "FLIGHT_SERVER"
        },
        {
          "name": "jsonl-server",
          "in": "flag",
          "description": "Enable JSON Lines Server. This provides a simple HTTP interface for querying data (default port 1603). Accepts SQL queries via POST requests and returns results in JSON Lines format.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "JSONL_SERVER"
        },
        {
          "name": "admin-server",
          "in": "flag",
          "description": "Enable Admin API Server. This provides management and administrative endpoints (default port 1610) for monitoring and controlling the nozzle system.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "ADMIN_SERVER"
        }
      ],
      "responses": {
        "0": {
          "description": "Server and embedded worker shut down gracefully"
        },
        "1": {
          "description": "Error occurred during operation or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "nozzle --config config.toml dev",
          "description": "Start all three servers with an embedded worker for local development"
        },
        {
          "command": "nozzle --config config.toml dev --flight-server",
          "description": "Start only the Arrow Flight RPC Server with an embedded worker"
        },
        {
          "command": "nozzle --config config.toml dev --flight-server --admin-server",
          "description": "Start Flight and Admin servers with an embedded worker"
        }
      ]
    },
    "server": {
      "summary": "Start query servers",
      "description": "Starts one or more query servers for serving blockchain data through different protocols. This command starts ONLY the query servers without any embedded worker - extraction jobs must be handled by separate worker processes or serverless dump commands. By default, all three servers (Arrow Flight, JSON Lines, and Admin API) are started. Servers can be selectively enabled using flags. If no server flags (--flight-server, --jsonl-server, --admin-server) are specified, ALL THREE servers are enabled by default. When any server flag is specified, only the explicitly enabled servers will start. Default ports: Arrow Flight (1602), JSON Lines (1603), Admin API (1610). The server runs continuously until terminated (Ctrl+C or kill signal).",
      "parameters": [
        {
          "name": "flight-server",
          "in": "flag",
          "description": "Enable Arrow Flight RPC Server. This provides a high-performance binary protocol for querying data (default port 1602). Uses Apache Arrow Flight for efficient data transfer.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "FLIGHT_SERVER"
        },
        {
          "name": "jsonl-server",
          "in": "flag",
          "description": "Enable JSON Lines Server. This provides a simple HTTP interface for querying data (default port 1603). Accepts SQL queries via POST requests and returns results in JSON Lines format.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "JSONL_SERVER"
        },
        {
          "name": "admin-server",
          "in": "flag",
          "description": "Enable Admin API Server. This provides management and administrative endpoints (default port 1610) for monitoring and controlling the nozzle system.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "ADMIN_SERVER"
        }
      ],
      "responses": {
        "0": {
          "description": "Server shut down gracefully"
        },
        "1": {
          "description": "Error occurred during server operation or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "nozzle --config config.toml server",
          "description": "Start all three servers (Arrow Flight, JSON Lines, and Admin API)"
        },
        {
          "command": "nozzle --config config.toml server --flight-server",
          "description": "Start only the Arrow Flight RPC Server"
        },
        {
          "command": "nozzle --config config.toml server --flight-server --jsonl-server",
          "description": "Start only Flight and JSON Lines servers"
        }
      ]
    },
    "dump": {
      "summary": "Extract data from blockchain sources to Parquet",
      "description": "Extracts blockchain data from various sources (EVM RPC, Firehose, Substreams) and saves it as Parquet files. Supports parallel extraction with configurable workers, dependency resolution for SQL datasets, and resumable extraction with progress tracking. Can run once or continuously on a schedule. Extraction is resumable - if interrupted, it will continue from where it left off. By default, dependencies are resolved and dumped in the correct order. Parallel jobs divide the block range into equal contiguous sections. When using --run-every-mins, the process runs indefinitely until terminated. The --fresh flag will delete existing progress and start from scratch. The --only-finalized-blocks flag only applies to raw datasets, not SQL-derived ones. Progress is tracked in the metadata database for resumability.",
      "parameters": [
        {
          "name": "dataset",
          "in": "option",
          "description": "The name or path of the dataset to dump. This will be looked up in the dataset definition directory and used as a subdirectory in the output path (<data_dir>/<dataset>). Accepts a comma-separated list of datasets, which will be dumped with their dependencies resolved unless --ignore-deps is set. Also accepts paths to .json manifest files.",
          "required": true,
          "arity": {
            "min": 1
          },
          "schema": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "x-environment": "DUMP_DATASET",
          "x-value-delimiter": ","
        },
        {
          "name": "ignore-deps",
          "in": "flag",
          "description": "If set to true, only the listed datasets will be dumped in the order they are listed. By default, the command dumps listed datasets and their dependencies, ordered such that each dataset will be dumped after all datasets they depend on.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "DUMP_IGNORE_DEPS"
        },
        {
          "name": "end-block",
          "in": "option",
          "alias": ["e"],
          "description": "The block number to end at, inclusive. If omitted, defaults to a recent block. If the value starts with '-', it is interpreted as relative to the latest block for this dataset (e.g., -100 means 100 blocks before the latest).",
          "required": false,
          "schema": {
            "type": "integer"
          },
          "x-environment": "DUMP_END_BLOCK"
        },
        {
          "name": "n-jobs",
          "in": "option",
          "alias": ["j"],
          "description": "How many parallel extractor jobs to run. Defaults to 1. Each job will be responsible for an equal number of blocks. Example: If start = 0, end = 10,000,000 and n_jobs = 10, then each job will be responsible for a contiguous section of 1 million blocks.",
          "required": false,
          "schema": {
            "type": "integer",
            "default": 1,
            "minimum": 1
          },
          "x-environment": "DUMP_N_JOBS"
        },
        {
          "name": "partition-size-mb",
          "in": "option",
          "description": "The size of each partition in MB. Once the size is reached, a new part file is created. This is based on the estimated in-memory size of the data. The actual on-disk file size will vary, but will correlate with this value. Defaults to 4 GB (4096 MB).",
          "required": false,
          "schema": {
            "type": "integer",
            "default": 4096,
            "minimum": 1
          },
          "x-environment": "DUMP_PARTITION_SIZE_MB"
        },
        {
          "name": "run-every-mins",
          "in": "option",
          "description": "How often to run the dump job in minutes. By default, the command will run once and exit. When specified, the dump will run continuously, executing every N minutes to keep the data up to date.",
          "required": false,
          "schema": {
            "type": "integer",
            "minimum": 1
          },
          "x-environment": "DUMP_RUN_EVERY_MINS"
        },
        {
          "name": "location",
          "in": "option",
          "description": "The location of the dump. If not specified, the dump will be written to the default location in NOZZLE_DATA_DIR as configured in the config file. Can be a local path or cloud storage URL (S3, GCS, Azure).",
          "required": false,
          "schema": {
            "type": "string"
          }
        },
        {
          "name": "fresh",
          "in": "flag",
          "description": "Overwrite existing location and dump to a new, fresh directory. This removes any existing progress and starts the extraction from the beginning.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "DUMP_FRESH"
        },
        {
          "name": "only-finalized-blocks",
          "in": "flag",
          "description": "Only dump finalized block data. This only applies to raw datasets (not SQL-derived datasets) and ensures that only blocks that have been finalized by the blockchain are extracted, avoiding potential reorgs.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "DUMP_ONLY_FINALIZED_BLOCKS"
        }
      ],
      "responses": {
        "0": {
          "description": "Dump completed successfully (or continuing if --run-every-mins is set)"
        },
        "1": {
          "description": "Error occurred during dump or configuration/dataset is invalid"
        }
      },
      "x-examples": [
        {
          "command": "nozzle --config config.toml dump --dataset eth_firehose -e 4000000 -j 2",
          "description": "Dump eth_firehose dataset up to block 4,000,000 using 2 parallel jobs"
        },
        {
          "command": "nozzle --config config.toml dump --dataset eth_rpc,polygon_firehose",
          "description": "Dump multiple datasets with dependency resolution"
        },
        {
          "command": "nozzle --config config.toml dump --dataset eth_firehose --fresh --only-finalized-blocks",
          "description": "Start a fresh dump with only finalized blocks"
        }
      ]
    }
  }
}
