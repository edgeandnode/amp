use std::{any::Any, borrow::Cow, collections::BTreeMap, path::PathBuf, sync::Arc};

use datafusion::{
    arrow::{compute::SortOptions, datatypes::SchemaRef},
    catalog::{Session, TableProvider},
    common::{
        stats::Precision, Column as LogicalColumn, ColumnStatistics, Constraints, Statistics,
    },
    datasource::{listing::PartitionedFile, TableType},
    error::{DataFusionError, Result as DataFusionResult},
    logical_expr::{Expr as LogicalExpr, LogicalPlan, SortExpr, TableProviderFilterPushDown},
    parquet::arrow::async_reader::ParquetObjectReader,
    physical_expr::PhysicalSortExpr,
    physical_plan::{expressions::Column, ExecutionPlan},
    sql::TableReference,
};
use futures::{future::BoxFuture, stream::BoxStream, FutureExt, StreamExt, TryStreamExt};
use metadata_db::{FileMetadata as NozzleFileMetadata, LocationId, MetadataDb, TableId};
use object_store::{
    path::{Error as PathError, Path},
    ObjectMeta, ObjectStore,
};
use url::Url;

use crate::{
    catalog::{
        logical::Table as LogicalTable,
        physical::dump::{split_files, BlockRange},
    },
    BoxError, Store, BLOCK_NUM,
};

use super::dump::{validate_name, DumpListingTable};

/// A table in the Nozzle catalog. This is a wrapper around the
/// [`DumpListingTable`] that is used to access the files
/// generated by Nozzle dump. It provides a unified interface for
/// accessing the files, regardless of whether there is a Metadata
/// provider (a [`MetadataDb`]) present or not.
///
/// ## Variants
/// - [Local](PhysicalTable::Local): A table that is backed by files generated by Nozzle dump.
/// - [Optimized](PhysicalTable::Optimized): A table that is backed by files generated by Nozzle dump,
///   optimized with an external Metadata provider.
#[derive(Clone, Debug)]
pub enum PhysicalTable {
    /// A [`DumpListingTable`] that is backed by files generated by Nozzle dump.
    Local {
        /// The [`DumpListingTable`] that is backed by files generated by Nozzle dump.
        dump: Arc<DumpListingTable>,
        /// The [`ObjectStore`] that is used to access the files.
        object_store: Arc<dyn ObjectStore>,
    },
    /// A [`DumpListingTable`] that is backed by files generated by Nozzle dump,
    /// This table is optimized with an external Metadata provider
    Optimized {
        /// The [`DumpListingTable`] that is backed by files generated by Nozzle dump.
        dump: Arc<DumpListingTable>,
        /// The [`ObjectStore`] that is used to access the files.
        object_store: Arc<dyn ObjectStore>,
        /// The [`MetadataDb`] that is used to optimize the table.
        metadata_provider: Arc<MetadataDb>,
        /// The location ID of the table in the metadata provider.
        location_id: LocationId,
    },
    // /// For memory tables, streaming tables, or other tables that are
    // /// not backed by an object store but may be present in a nozzle
    // /// dataset. These may be used for cached statistics or metadata
    // /// providers for dump tables.
    // Other {
    //     /// The table provider that is not backed by an object store.
    //     table_provider: Arc<dyn TableProvider>,
    //     /// The logical table that is not backed by an object store.
    //     logical_table: Arc<LogicalTable>,
    //     /// The table reference for the table that is not backed by an object store.
    //     table_ref: Arc<TableReference>,
    // },
}

/// Methods for creating a [`PhysicalTable`]
impl PhysicalTable {
    pub fn try_at_active_location(
        dataset_name: &str,
        logical_table: &LogicalTable,
        url: Url,
        location_id: LocationId,
        object_store: Arc<dyn ObjectStore>,
        metadata_provider: impl AsRef<MetadataDb>,
    ) -> Result<Self, BoxError> {
        let metadata_provider = Arc::new(metadata_provider.as_ref().clone());
        let dataset_version = url
            .path_segments()
            .ok_or(PathError::InvalidPath {
                path: PathBuf::from(url.path()),
            })
            .map(|mut s| {
                s.nth_back(2)
                    .filter(|segment| *segment != dataset_name)
                    .map(ToString::to_string)
            })?;
        let dump = DumpListingTable::new(dataset_name, dataset_version, logical_table, url)?;

        let table = Self::Optimized {
            dump: Arc::new(dump),
            object_store: object_store.clone(),
            metadata_provider: metadata_provider.clone(),
            location_id,
        };

        Ok(table)
    }

    pub async fn try_next_revision(
        logical_table: &LogicalTable,
        data_store: &Store,
        dataset_name: &str,
        metadata_provider: impl AsRef<MetadataDb>,
    ) -> Result<Self, BoxError> {
        let metadata_provider = Arc::new(metadata_provider.as_ref().clone());

        let table_id = TableId {
            dataset: dataset_name,
            dataset_version: None,
            table: &logical_table.name,
        };

        let path = make_location_path(table_id);
        let url = data_store.url().join(&path)?;
        let location_id = metadata_provider
            .register_location(table_id, data_store.bucket(), &path, &url, false)
            .await?;
        metadata_provider
            .set_active_location(table_id, url.as_str())
            .await?;

        let object_store = data_store.object_store();

        Self::try_at_active_location(
            dataset_name,
            logical_table,
            url,
            location_id,
            object_store,
            metadata_provider,
        )
    }

    pub(super) fn try_at_static_location(
        store: Arc<Store>,
        dataset_name: &str,
        logical_table: LogicalTable,
    ) -> Result<Self, BoxError> {
        validate_name(&logical_table.name)?;
        let dataset_version = None;
        let table_id = TableId {
            dataset: dataset_name,
            dataset_version,
            table: &logical_table.name,
        };
        let input = format!("{}/{}/", dataset_name, table_id.table);
        let url = store.url().join(&input)?;

        let dump = DumpListingTable::new(dataset_name, None, &logical_table, url)?;

        let table = Self::Local {
            dump: Arc::new(dump),
            object_store: store.object_store(),
        };

        Ok(table)
    }
}

impl PhysicalTable {
    pub fn logical_table(&self) -> LogicalTable {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.logical_table(),
            // Other { logical_table, .. } => logical_table.as_ref().clone(),
        }
    }
    pub fn table_ref(&self) -> Arc<TableReference> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.table_ref(),
        }
    }

    pub fn catalog_schema(&self) -> String {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.dataset.clone(),
        }
    }

    pub fn network(&self) -> Option<String> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.network.clone(),
        }
    }

    pub fn table_id(&self) -> TableId<'_> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.table_id(),
        }
    }

    pub fn table_name(&self) -> &str {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.table_name(),
        }
    }

    pub fn url(&self) -> Option<Url> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => Some(dump.url().clone()),
        }
    }

    pub fn path(&self) -> DataFusionResult<Path> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => Ok(dump.path().clone()),
        }
    }

    pub fn object_store(&self) -> DataFusionResult<Arc<dyn ObjectStore>> {
        use PhysicalTable::*;
        match self {
            Local { object_store, .. } | Optimized { object_store, .. } => Ok(object_store.clone()),
        }
    }

    pub fn is_meta(&self) -> bool {
        use PhysicalTable::*;
        match self {
            Local { .. } | Optimized { .. } => false,
        }
    }

    pub fn location_id(&self) -> Option<LocationId> {
        use PhysicalTable::*;
        match self {
            Optimized { location_id, .. } => Some(*location_id),
            _ => None,
        }
    }

    pub fn order_exprs(&self) -> Vec<Vec<SortExpr>> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => {
                let physical_order_exprs = dump.order_exprs.clone();

                physical_order_exprs
                    .iter()
                    .map(|sort_exprs| {
                        sort_exprs
                            .iter()
                            .map(
                                |PhysicalSortExpr {
                                     expr,
                                     options:
                                         SortOptions {
                                             descending,
                                             nulls_first,
                                         },
                                 }| {
                                    let expr = expr
                                        .as_any()
                                        .downcast_ref::<Column>()
                                        .cloned()
                                        .map(|col| {
                                            LogicalExpr::Column(LogicalColumn::new(
                                                Some(self.table_ref().as_ref().clone()),
                                                col.name(),
                                            ))
                                        })
                                        .expect("PhysicalSortExpr::expr should be a Column");

                                    SortExpr::new(expr, !*descending, *nulls_first)
                                },
                            )
                            .collect()
                    })
                    .collect()
            }
        }
    }

    pub fn physical_sort_exprs(&self) -> Vec<Vec<PhysicalSortExpr>> {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.order_exprs.clone(),
        }
    }

    pub fn ranges(&self) -> DataFusionResult<BoxFuture<'_, DataFusionResult<Vec<(u64, u64)>>>> {
        Ok(self.stream_scanned_ranges()?.try_collect().boxed())
    }

    pub fn parquet_files(
        &self,
    ) -> DataFusionResult<BoxFuture<'_, DataFusionResult<BTreeMap<String, ObjectMeta>>>> {
        Ok(self
            .stream_files()?
            .try_collect::<BTreeMap<String, ObjectMeta>>()
            .boxed())
    }
}

/// Streaming methods for the [`PhysicalTable`].
impl PhysicalTable {
    fn stream_scanned_ranges(
        &self,
    ) -> DataFusionResult<BoxStream<'_, DataFusionResult<(u64, u64)>>> {
        use PhysicalTable::*;
        match self {
            Local { dump, object_store } => Ok(dump.ranges_stream(object_store)),
            Optimized {
                dump,
                metadata_provider,
                ..
            } => {
                let tbl = dump.table_id();
                Ok(metadata_provider
                    .stream_ranges(tbl)
                    .map_err(|e| DataFusionError::External(Box::new(e)))
                    .map_ok(|(start, end)| (start as u64, end as u64))
                    .boxed())
            }
        }
    }

    fn stream_files(
        &self,
    ) -> DataFusionResult<BoxStream<'_, DataFusionResult<(String, ObjectMeta)>>> {
        use PhysicalTable::*;
        match self {
            Local { dump, object_store } => Ok(dump.stream_files(object_store)),
            Optimized {
                dump,
                metadata_provider,
                ..
            } => {
                let tbl = dump.table_id();
                Ok(metadata_provider
                    .stream_object_meta(tbl)
                    .map_err(|e| DataFusionError::External(Box::new(e)))
                    .boxed())
            }
        }
    }

    pub fn stream_object_readers(
        &self,
    ) -> DataFusionResult<BoxStream<'_, DataFusionResult<(ParquetObjectReader, ObjectMeta)>>> {
        use PhysicalTable::*;
        match self {
            Local { dump, object_store } => Ok(dump.object_reader_stream(object_store).boxed()),
            Optimized { .. } => Err(DataFusionError::Internal(
                "PhysicalTable::stream_object_readers: Optimized table not supported".to_string(),
            )),
        }
    }

    fn stream_nozzle_metadata(
        &self,
    ) -> DataFusionResult<BoxStream<'_, DataFusionResult<NozzleFileMetadata>>> {
        use PhysicalTable::*;
        match self {
            Local {
                dump, object_store, ..
            } => Ok(dump.nozzle_metadata_stream(object_store)),
            Optimized {
                dump,
                metadata_provider,
                ..
            } => Ok(metadata_provider
                .stream_nozzle_metadata(dump.table_id())
                .map_err(|e| DataFusionError::External(Box::new(e)))
                .boxed()),
        }
    }

    async fn list_files_for_scan<'a>(
        &'a self,
        _filters: &'a [LogicalExpr],
        _limit: Option<usize>,
    ) -> DataFusionResult<(Vec<Vec<PartitionedFile>>, Statistics)> {
        use Precision::*;
        // Key: range_start
        // Value: (range_end, PartitionedFile)
        let mut files: BTreeMap<BlockRange, PartitionedFile> = BTreeMap::new();

        let mut table_stats = Statistics::new_unknown(self.schema().as_ref());
        let mut stream = self.stream_nozzle_metadata()?;

        let block_num_idx = self.schema().index_of(BLOCK_NUM)?;

        while let Some(NozzleFileMetadata {
            object_meta,
            range: (range_start, range_end),
            row_count,
            data_size,
            size_hint,
        }) = stream.try_next().await?
        {
            // Check if the new file is contained by an existing file
            if files
                .keys()
                .any(|BlockRange(start, end)| range_start >= *start && range_end <= *end)
            {
                continue;
            }

            // Drop files that are contained by the new file
            let drop = files
                .keys()
                .filter(|BlockRange(start, end)| {
                    (range_start == *start && range_end > *end)
                        || (range_start > *start && range_end <= *end)
                })
                .cloned()
                .collect::<Vec<_>>();

            for idx in drop {
                files.remove(&idx);
            }

            let block_num_stats = ColumnStatistics {
                null_count: Exact(0),
                max_value: Inexact(range_end.into()),
                min_value: Inexact(range_start.into()),
                ..Default::default()
            };

            let mut file_stats = Statistics::new_unknown(&self.schema().clone());

            file_stats
                .column_statistics
                .insert(block_num_idx, block_num_stats);

            if let Some(num_rows) = row_count {
                file_stats.num_rows = Exact(num_rows as usize);
            }

            if let Some(total_size_bytes) = data_size {
                file_stats.total_byte_size = Exact(total_size_bytes as usize);
            }
            let partitioned_file = PartitionedFile {
                object_meta,
                statistics: Some(file_stats.clone()),
                metadata_size_hint: size_hint.map(|s| s as usize),
                partition_values: Vec::new(),
                range: None,
                extensions: None,
            };
            files.insert((range_start, range_end).into(), partitioned_file.clone());
        }

        let split_files = split_files(&mut table_stats, &[block_num_idx], files, 10)?;

        Ok((split_files, table_stats))
    }
}

impl TryFrom<Arc<dyn TableProvider>> for PhysicalTable {
    type Error = DataFusionError;
    fn try_from(dyn_table: Arc<dyn TableProvider>) -> Result<Self, Self::Error> {
        if let Some(table) = dyn_table.as_ref().as_any().downcast_ref::<PhysicalTable>() {
            Ok(table.clone())
        } else {
            Err(DataFusionError::Internal(format!(
                "Failed to downcast to Table for table: {:?}",
                dyn_table.type_id()
            )))
        }
    }
}

impl TableProvider for PhysicalTable {
    fn as_any(&self) -> &dyn Any {
        self
    }

    fn schema(&self) -> SchemaRef {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump.schema.clone(),
        }
    }

    fn table_type(&self) -> TableType {
        TableType::Base
    }

    // Not using `async_trait` here as this is a wrapper for the
    // `scan` method of the underlying table provider.
    fn scan<'table, 'state, 'projection, 'filters, 'async_trait>(
        &'table self,
        state: &'state dyn Session,
        projection: Option<&'projection Vec<usize>>,
        filters: &'filters [LogicalExpr],
        limit: Option<usize>,
    ) -> BoxFuture<'async_trait, DataFusionResult<Arc<dyn ExecutionPlan>>>
    where
        'table: 'async_trait,
        'state: 'async_trait,
        'projection: 'async_trait,
        'filters: 'async_trait,
        Self: 'async_trait,
    {
        use PhysicalTable::*;
        match self {
            Local { dump, .. } | Optimized { dump, .. } => dump
                .scan(
                    state,
                    projection,
                    filters,
                    limit,
                    self.list_files_for_scan(filters, limit).boxed(),
                )
                .boxed(),
        }
    }

    fn get_table_definition(&self) -> Option<&str> {
        None
    }

    fn constraints(&self) -> Option<&Constraints> {
        None
    }

    fn get_column_default(&self, _column: &str) -> Option<&LogicalExpr> {
        None
    }

    fn get_logical_plan(&self) -> Option<Cow<LogicalPlan>> {
        None
    }

    fn statistics(&self) -> Option<Statistics> {
        None
    }

    fn supports_filters_pushdown(
        &self,
        filters: &[&LogicalExpr],
    ) -> DataFusionResult<Vec<datafusion::logical_expr::TableProviderFilterPushDown>> {
        use PhysicalTable::*;
        match self {
            Local { .. } | Optimized { .. } => Ok(vec![
                TableProviderFilterPushDown::Unsupported;
                filters.len()
            ]),
        }
    }
}

// The path format is: `<dataset>/[<version>/]<table>/<UUIDv7>/`
pub fn make_location_path(table_id: TableId<'_>) -> String {
    let mut path = String::new();
    // Add dataset
    path.push_str(table_id.dataset);
    path.push('/');

    // Add version if present
    if let Some(version) = table_id.dataset_version {
        path.push_str(version);
        path.push('/');
    }

    // Add table
    path.push_str(table_id.table);
    path.push('/');

    // Add UUIDv7
    let uuid = uuid::Uuid::now_v7();
    path.push_str(&uuid.to_string());
    path.push('/');

    path
}
