-- Migration: Split physical_tables into physical_tables (meta) and physical_table_revisions
--
-- This migration decouples table identity from storage paths, enabling:
-- - Data sharing across datasets with same spec
-- - External data import (link any directory to any table)
--
-- Changes:
-- 1. Create physical_table_revisions table (preserves existing IDs for FK compatibility)
-- 2. Update file_metadata and gc_manifest FKs to point to revisions
-- 3. Transform physical_tables into a meta table with active_revision_id pointer
-- 4. Populate relationships between tables

-- =============================================================================
-- STEP 1: Create physical_table_revisions table
-- =============================================================================
-- Using GENERATED BY DEFAULT to allow explicit ID insertion during migration
-- This preserves location_id values that file_metadata and gc_manifest reference

CREATE TABLE physical_table_revisions (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    created_at TIMESTAMP NOT NULL DEFAULT (now() AT TIME ZONE 'utc'),
    updated_at TIMESTAMP NOT NULL DEFAULT (now() AT TIME ZONE 'utc'),
    path TEXT NOT NULL UNIQUE,
    writer BIGINT REFERENCES jobs(id) ON DELETE SET NULL,
    metadata JSONB NOT NULL DEFAULT ('{}'::jsonb)
);

-- =============================================================================
-- STEP 2: Copy revision data with existing IDs
-- =============================================================================
-- Critical: This preserves the location_id values that file_metadata references

INSERT INTO physical_table_revisions (id, created_at, updated_at, path, writer)
SELECT id, created_at, created_at AS updated_at, path, writer
FROM physical_tables;

-- Reset the identity sequence to continue from max ID (only if table has rows)
DO $$
BEGIN
    IF EXISTS (SELECT 1 FROM physical_table_revisions LIMIT 1) THEN
        PERFORM setval(
            pg_get_serial_sequence('physical_table_revisions', 'id'),
            (SELECT MAX(id) FROM physical_table_revisions)
        );
    END IF;
END $$;

-- =============================================================================
-- STEP 3: Update file_metadata FK to point to physical_table_revisions
-- =============================================================================

ALTER TABLE file_metadata
    DROP CONSTRAINT IF EXISTS file_metadata_location_id_fkey;

ALTER TABLE file_metadata
    ADD CONSTRAINT file_metadata_location_id_fkey
    FOREIGN KEY (location_id)
    REFERENCES physical_table_revisions(id)
    ON DELETE CASCADE;

-- =============================================================================
-- STEP 4: Update gc_manifest FK to point to physical_table_revisions
-- =============================================================================

ALTER TABLE gc_manifest
    DROP CONSTRAINT IF EXISTS gc_manifest_location_id_fkey;

ALTER TABLE gc_manifest
    ADD CONSTRAINT gc_manifest_location_id_fkey
    FOREIGN KEY (location_id)
    REFERENCES physical_table_revisions(id)
    ON DELETE CASCADE;

-- =============================================================================
-- STEP 5: Rename old table and create new physical_tables (meta)
-- =============================================================================

ALTER TABLE physical_tables RENAME TO physical_tables_old;

CREATE TABLE physical_tables (
    id BIGINT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    created_at TIMESTAMP NOT NULL DEFAULT (now() AT TIME ZONE 'utc'),
    updated_at TIMESTAMP NOT NULL DEFAULT (now() AT TIME ZONE 'utc'),
    dataset_namespace TEXT NOT NULL,
    dataset_name TEXT NOT NULL,
    table_name TEXT NOT NULL,
    manifest_hash TEXT NOT NULL,
    active_revision_id BIGINT REFERENCES physical_table_revisions(id) ON DELETE SET NULL
);

-- =============================================================================
-- STEP 6: Populate physical_tables (meta) from old data
-- =============================================================================
-- Insert one row per unique (dataset_namespace, dataset_name, table_name, manifest_hash) combination
-- Set active_revision_id to the revision that was marked active (if any)

INSERT INTO physical_tables (
    created_at,
    updated_at,
    dataset_namespace,
    dataset_name,
    table_name,
    manifest_hash,
    active_revision_id
)
SELECT
    MIN(created_at) AS created_at,
    MIN(created_at) AS updated_at,
    dataset_namespace,
    dataset_name,
    table_name,
    manifest_hash,
    (SELECT id FROM physical_tables_old pt2
     WHERE pt2.manifest_hash = physical_tables_old.manifest_hash
       AND pt2.table_name = physical_tables_old.table_name
       AND pt2.active = true
     LIMIT 1) AS active_revision_id
FROM physical_tables_old
GROUP BY dataset_namespace, dataset_name, table_name, manifest_hash;

-- =============================================================================
-- STEP 7: Create indexes
-- =============================================================================

-- Unique constraint: one physical_table per (dataset_namespace, dataset_name, manifest_hash, table_name)
CREATE UNIQUE INDEX unique_manifest_table
ON physical_tables (dataset_namespace, dataset_name, manifest_hash, table_name);

-- Index for active_revision_id
CREATE INDEX idx_physical_tables_active_revision_id
  ON physical_tables (active_revision_id) WHERE active_revision_id IS NOT NULL;

-- =============================================================================
-- STEP 8: Cleanup
-- =============================================================================

-- Drop the old partial unique index (no longer needed)
DROP INDEX IF EXISTS unique_active_per_manifest_table;

-- Drop the old table
DROP TABLE physical_tables_old;
