{
  "opencli": "1.0.0",
  "info": {
    "title": "ampd - Amp Daemon",
    "description": "High-performance ETL (Extract, Transform, Load) architecture for blockchain data services. Extracts blockchain data from various sources, transforms it via SQL queries, and serves it through multiple query interfaces.",
    "version": "0.1.0-20251021",
    "contact": {
      "name": "Edge & Node",
      "url": "https://github.com/edgeandnode/amp"
    }
  },
  "environment": [
    {
      "name": "AMP_CONFIG",
      "description": "The configuration file to use. This file defines where to look for dataset definitions and providers, along with many other configuration options."
    }
  ],
  "commands": {
    "migrate": {
      "summary": "Run migrations on the metadata database",
      "description": "Executes database schema migrations on the PostgreSQL metadata database. This command requires the --config parameter to be set, which should point to a configuration file containing the metadata database connection details. Migrations are applied automatically and cannot be rolled back through this command.",
      "parameters": [],
      "responses": {
        "0": {
          "description": "Migrations completed successfully"
        },
        "1": {
          "description": "Error occurred during migration or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "ampd --config config.toml migrate",
          "description": "Run migrations using the specified configuration file"
        }
      ]
    },
    "worker": {
      "summary": "Run distributed worker node",
      "description": "Starts a worker node that connects to the metadata database and processes distributed dump tasks. The worker polls for available jobs and executes them. Multiple workers can run in parallel to process different portions of data extraction jobs. The node-id must be unique across all running workers. Workers require access to the metadata database configured in the config file and run continuously until terminated.",
      "parameters": [
        {
          "name": "node-id",
          "in": "option",
          "description": "The unique identifier for this worker node. Used to track worker status and job assignments in the distributed system.",
          "required": true,
          "schema": {
            "type": "string"
          },
          "x-environment": "AMP_NODE_ID"
        }
      ],
      "responses": {
        "0": {
          "description": "Worker shut down gracefully"
        },
        "1": {
          "description": "Error occurred during worker operation or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "ampd --config config.toml worker --node-id worker-01",
          "description": "Start a worker node with ID 'worker-01'"
        },
        {
          "command": "AMP_CONFIG=config.toml AMP_NODE_ID=worker-02 ampd worker",
          "description": "Start a worker using environment variables"
        }
      ]
    },
    "controller": {
      "summary": "Start controller service",
      "description": "Starts the controller service that provides the Admin API for managing Amp operations. The controller runs as a standalone service separate from query servers, enabling independent deployment and scaling of management operations. Default port: Admin API (1610). The controller provides endpoints for dataset management, job control, worker coordination, and file operations. It runs continuously until terminated (Ctrl+C or kill signal).",
      "parameters": [],
      "responses": {
        "0": {
          "description": "Controller shut down gracefully"
        },
        "1": {
          "description": "Error occurred during controller operation or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "ampd --config config.toml controller",
          "description": "Start the controller with Admin API"
        }
      ]
    },
    "server": {
      "summary": "Start query servers",
      "description": "Starts one or more query servers for serving blockchain data through different protocols. By default, both query servers (Arrow Flight and JSON Lines) are started. Servers can be selectively enabled using flags. If no server flags (--flight-server, --jsonl-server) are specified, BOTH servers are enabled by default. When any server flag is specified, only the explicitly enabled servers will start. Default ports: Arrow Flight (1602), JSON Lines (1603). The server runs continuously until terminated (Ctrl+C or kill signal).",
      "parameters": [
        {
          "name": "flight-server",
          "in": "flag",
          "description": "Enable Arrow Flight RPC Server. This provides a high-performance binary protocol for querying data (default port 1602). Uses Apache Arrow Flight for efficient data transfer.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "FLIGHT_SERVER"
        },
        {
          "name": "jsonl-server",
          "in": "flag",
          "description": "Enable JSON Lines Server. This provides a simple HTTP interface for querying data (default port 1603). Accepts SQL queries via POST requests and returns results in JSON Lines format.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "JSONL_SERVER"
        }
      ],
      "responses": {
        "0": {
          "description": "Server shut down gracefully"
        },
        "1": {
          "description": "Error occurred during server operation or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "ampd --config config.toml server",
          "description": "Start both query servers (Arrow Flight and JSON Lines)"
        },
        {
          "command": "ampd --config config.toml server --flight-server",
          "description": "Start only the Arrow Flight RPC Server"
        },
        {
          "command": "ampd --config config.toml server --jsonl-server",
          "description": "Start only the JSON Lines Server"
        }
      ]
    },
    "dev": {
      "summary": "Start development mode with embedded worker and query servers",
      "description": "Starts a combined server, controller, and worker in a single process for simplified local development and testing. By default, all three components (Arrow Flight, JSON Lines, and Admin API) are started along with an embedded worker. Components can be selectively enabled using flags. If no flags (--flight-server, --jsonl-server, --admin-server) are specified, ALL THREE are enabled by default. When any flag is specified, only the explicitly enabled components will start. The worker always runs with node ID 'worker'. Default ports: Arrow Flight (1602), JSON Lines (1603), Admin API (1610). Dev mode allows temporary database configuration for quick local testing. The process runs continuously until terminated (Ctrl+C or kill signal).",
      "parameters": [
        {
          "name": "flight-server",
          "in": "flag",
          "description": "Enable Arrow Flight RPC Server. This provides a high-performance binary protocol for querying data (default port 1602). Uses Apache Arrow Flight for efficient data transfer.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "FLIGHT_SERVER"
        },
        {
          "name": "jsonl-server",
          "in": "flag",
          "description": "Enable JSON Lines Server. This provides a simple HTTP interface for querying data (default port 1603). Accepts SQL queries via POST requests and returns results in JSON Lines format.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "JSONL_SERVER"
        },
        {
          "name": "admin-server",
          "in": "flag",
          "description": "Enable Admin API Server. This provides management and administrative endpoints (default port 1610) for monitoring and controlling the amp system.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "ADMIN_SERVER"
        }
      ],
      "responses": {
        "0": {
          "description": "Dev mode shut down gracefully"
        },
        "1": {
          "description": "Error occurred during dev mode operation or configuration is invalid"
        }
      },
      "x-examples": [
        {
          "command": "ampd --config config.toml dev",
          "description": "Start dev mode with all components (query servers, Admin API, and embedded worker)"
        },
        {
          "command": "ampd --config config.toml dev --flight-server --admin-server",
          "description": "Start dev mode with only Arrow Flight server and Admin API (no JSON Lines server)"
        },
        {
          "command": "ampd --config config.toml dev --jsonl-server",
          "description": "Start dev mode with only JSON Lines server (no Flight server or Admin API)"
        }
      ]
    },
    "dump": {
      "summary": "Extract data from blockchain sources to Parquet",
      "description": "Extracts blockchain data from various sources (EVM RPC, Firehose) and saves it as Parquet files. Supports parallel extraction with configurable workers, dependency resolution for SQL datasets, and resumable extraction with progress tracking. Can run once or continuously on a schedule. Extraction is resumable - if interrupted, it will continue from where it left off. By default, dependencies are resolved and dumped in the correct order. Parallel jobs divide the block range into equal contiguous sections. When using --run-every-mins, the process runs indefinitely until terminated. The --fresh flag will delete existing progress and start from scratch. The --only-finalized-blocks flag only applies to raw datasets, not SQL-derived ones. Progress is tracked in the metadata database for resumability.",
      "parameters": [
        {
          "name": "dataset",
          "in": "option",
          "description": "The name or path of the dataset to dump. This will be looked up in the dataset definition directory and used as a subdirectory in the output path (<data_dir>/<dataset>). Accepts a comma-separated list of datasets, which will be dumped with their dependencies resolved unless --ignore-deps is set. Also accepts paths to .json manifest files.",
          "required": true,
          "arity": {
            "min": 1
          },
          "schema": {
            "type": "array",
            "items": {
              "type": "string"
            }
          },
          "x-environment": "DUMP_DATASET",
          "x-value-delimiter": ","
        },
        {
          "name": "ignore-deps",
          "in": "flag",
          "description": "If set to true, only the listed datasets will be dumped in the order they are listed. By default, the command dumps listed datasets and their dependencies, ordered such that each dataset will be dumped after all datasets they depend on.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "DUMP_IGNORE_DEPS"
        },
        {
          "name": "end-block",
          "in": "option",
          "alias": ["e"],
          "description": "The block number to end at, inclusive. If omitted, defaults to a recent block. If the value starts with '-', it is interpreted as relative to the latest block for this dataset (e.g., -100 means 100 blocks before the latest).",
          "required": false,
          "schema": {
            "type": "integer"
          },
          "x-environment": "DUMP_END_BLOCK"
        },
        {
          "name": "n-jobs",
          "in": "option",
          "alias": ["j"],
          "description": "How many parallel extractor jobs to run. Defaults to 1. Each job will be responsible for an equal number of blocks. Example: If start = 0, end = 10,000,000 and n_jobs = 10, then each job will be responsible for a contiguous section of 1 million blocks.",
          "required": false,
          "schema": {
            "type": "integer",
            "default": 1,
            "minimum": 1
          },
          "x-environment": "DUMP_N_JOBS"
        },
        {
          "name": "partition-size-mb",
          "in": "option",
          "description": "The size of each partition in MB. Once the size is reached, a new part file is created. This is based on the estimated in-memory size of the data. The actual on-disk file size will vary, but will correlate with this value. Defaults to 4 GB (4096 MB).",
          "required": false,
          "schema": {
            "type": "integer",
            "default": 4096,
            "minimum": 1
          },
          "x-environment": "DUMP_PARTITION_SIZE_MB"
        },
        {
          "name": "run-every-mins",
          "in": "option",
          "description": "How often to run the dump job in minutes. By default, the command will run once and exit. When specified, the dump will run continuously, executing every N minutes to keep the data up to date.",
          "required": false,
          "schema": {
            "type": "integer",
            "minimum": 1
          },
          "x-environment": "DUMP_RUN_EVERY_MINS"
        },
        {
          "name": "location",
          "in": "option",
          "description": "The location of the dump. If not specified, the dump will be written to the default location in AMP_DATA_DIR as configured in the config file. Can be a local path or cloud storage URL (S3, GCS, Azure).",
          "required": false,
          "schema": {
            "type": "string"
          }
        },
        {
          "name": "fresh",
          "in": "flag",
          "description": "Overwrite existing location and dump to a new, fresh directory. This removes any existing progress and starts the extraction from the beginning.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "DUMP_FRESH"
        },
        {
          "name": "only-finalized-blocks",
          "in": "flag",
          "description": "Only dump finalized block data. This only applies to raw datasets (not SQL-derived datasets) and ensures that only blocks that have been finalized by the blockchain are extracted, avoiding potential reorgs.",
          "required": false,
          "schema": {
            "type": "boolean",
            "default": false
          },
          "x-environment": "DUMP_ONLY_FINALIZED_BLOCKS"
        }
      ],
      "responses": {
        "0": {
          "description": "Dump completed successfully (or continuing if --run-every-mins is set)"
        },
        "1": {
          "description": "Error occurred during dump or configuration/dataset is invalid"
        }
      },
      "x-examples": [
        {
          "command": "ampd --config config.toml dump --dataset eth_firehose -e 4000000 -j 2",
          "description": "Dump eth_firehose dataset up to block 4,000,000 using 2 parallel jobs"
        },
        {
          "command": "ampd --config config.toml dump --dataset eth_rpc,polygon_firehose",
          "description": "Dump multiple datasets with dependency resolution"
        },
        {
          "command": "ampd --config config.toml dump --dataset eth_firehose --fresh --only-finalized-blocks",
          "description": "Start a fresh dump with only finalized blocks"
        }
      ]
    }
  }
}
