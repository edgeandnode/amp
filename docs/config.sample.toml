# Sample of a configuration. Copy this file and edit it. Set the environment variable `AMP_CONFIG`
# to its location. See the CONFIG.md for more context.
#
# Note that using a config file is not mandatory. You can alternatively provide some or all values
# through env vars. See the 'Using env vars' section of CONFIG.md for how to do this.
#
# When using filesystem paths, any relative paths will be resolved from the directory of this file.

# ================================
# Top-level configuration fields
# ================================

# Storage paths
# Where the extracted datasets are stored.
data_dir = "<config_dir/data>"

# Path to a providers directory. Each provider is configured as a separate toml file in this
# directory. Dataset definitions will reference providers by a relative path to this directory.
providers_dir = "<config_dir/providers>"

# Path to a directory containing dataset manifest files.
manifests_dir = "<config_dir/manifests>"

# Memory and performance
# max_mem_mb = 0            # Global memory limit for all queries in MB. 0 means unlimited (default: 0)
# query_max_mem_mb = 0      # Per-query memory limit in MB. 0 means unlimited per-query (default: 0)
# spill_location = []       # Paths for DataFusion temporary files for spill-to-disk (default: [])

# Operational timing
# poll_interval_secs = 1.0                 # Polling interval for new blocks during dump in seconds (default: 1.0)
# microbatch_max_interval = 100000         # Max interval for derived dataset dump microbatches in blocks (default: 100000)
# server_microbatch_max_interval = 1000    # Max interval for streaming server microbatches in blocks (default: 1000)

# Service addresses
# flight_addr = "0.0.0.0:1602"      # Arrow Flight RPC server address (default: "0.0.0.0:1602")
# jsonl_addr = "0.0.0.0:1603"       # JSON Lines server address (default: "0.0.0.0:1603")
# admin_api_addr = "0.0.0.0:1610"   # Admin API server address (default: "0.0.0.0:1610")

# ================================
# Table sections
# ================================

# Database configuration
# If not set, a temporary postgres DB will be provided.
# To opt out of this default behavior, set `ALLOW_TEMP_DB` to `false` in the environment.
# By default, temporary DBs will be created and persisted in the OS temp directory.
[metadata_db]
url = "postgres://<pg_url>"
# pool_size = 10       # Size of the connection pool (default: 10)
# auto_migrate = true  # Automatically run database migrations on startup (default: true)

# Observability
[opentelemetry]
# metrics_url = "http://localhost:4318/v1/metrics"     # Remote OpenTelemetry metrics collector endpoint (binary HTTP)
# metrics_export_interval_secs = 60.0                  # Interval in seconds to export metrics
# trace_url = "http://localhost:4318/v1/traces"        # Remote OpenTelemetry traces collector endpoint (HTTP)
# trace_ratio = 1.0                                    # Ratio of traces to sample (default: 1.0)

# Writer/Parquet configuration
[writer]
# compression = "zstd(1)"    # Compression algorithm: zstd, lz4, gzip, brotli, snappy, uncompressed (default: zstd(1))
# bloom_filters = false      # Enable bloom filters (default: false)
# cache_size_mb = 1024       # Parquet metadata cache size in MB (default: 1024)
# max_row_group_mb = 512     # Max row group size in MB (default: 512)

# Target partition size configuration
# Overflow is a tuning parameter that provides some flexibility for some cases,
# e.g. if two files that are both slightly larger than 50% of the size limit won't
# compact because their total exceeds the limit. Overflow provides some flexibility
# in this case. Additionally, the segment size algorithm is based on an approximation
# of the future compacted segment size which might end up being smaller or larger
# depending on factors like table width, value cardinality, compression ratios, etc.
# So adjusting Overflow provides a mechanism to reflect the realities of the data
# at rest while configuring the compaction algorithm.
# overflow = "1"         # Overflow multiplier: 1x target size (default: "1"), can use "1.5" for 1.5x, etc.
# bytes = 2147483648     # Target bytes per file - 2GB (default: 2147483648)
# rows = 0               # Target rows per file, 0 means no limit (default: 0)
# Note: nozzle dump command may optionally override bytes if partition_size_mb is provided
# or if the NOZZLE_PARTITION_SIZE_MB env var is set.

# File compactor
[writer.compactor]
# active = false                  # Enable or disable the compactor (default: false)
# metadata_concurrency = 2        # Max concurrent metadata operations (default: 2)
# write_concurrency = 2           # Max concurrent compaction write operations (default: 2)
# min_interval = 1.0              # Interval in seconds to run the compactor (default: 1.0)
# cooldown_duration = 1024.0      # Cooldown to wait before re-compacting a file, in seconds (default: 1024.0)
#
# Maximum generation for eager compaction, that is, cooldowns only apply for generations above this value.
# Default: 0. To disable eager compaction, set to -1.
# max_eager_generation = 0 

# Garbage collector
[writer.collector]
# active = false                   # Enable or disable the collector (default: false)
# min_interval = 30.0              # Interval in seconds to run the garbage collector (default: 30.0)
# deletion_lock_duration = 1800.0  # Duration in seconds to hold deletion lock on compacted files (default: 1800.0 = 30 minutes)
